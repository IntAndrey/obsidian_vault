---
title: "Как подключить к LibreChat RAG-систему, чтобы загружать .pdf, .docx, .xlsx и .pptx"
source: "https://habr.com/ru/companies/bothub/articles/956892/"
author:
  - "[[Дмитрий]]"
published: 2025-10-18
created: 2025-10-21
description: "Помню тот момент, когда я в очередной раз пытался вытащить конкретную спецификацию из стопки PDF‑отчетов. «Вот бы ИИ мог сам в этом покопаться», — подумал я. Это чувство..."
tags:
  - "clippings"
---
[![](https://habrastorage.org/getpro/habr/branding/fd0/52a/031/fd052a031080b9cfb20f96bc18d7e64e.png)](https://bothub.chat/?invitedBy=m_aGCkuyTgqllHCK0dUc7)

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/ab1/b2a/268/ab1b2a26894ca99a1a2f366cc494be46.jpg)

Помню тот момент, когда я в очередной раз пытался вытащить конкретную спецификацию из стопки PDF‑отчетов. «Вот бы ИИ мог сам в этом покопаться», — подумал я. Это чувство знакомо многим, кто работает с большими массивами текстовой информации.

Тогда я и решил, что хватит это терпеть. Последующий день превратился в марафон по установке и настройке RAG (генерация с дополнением извлеченной информацией). Это был путь проб и ошибок, который в итоге увенчался успехом. И теперь я хочу поделиться этим опытом с вами.

В этом материале мы:

- Пошагово установим **rag\_api** в уже развёрнутый LibreChat;
- Воспользуемся **Python 3.12**, **PostgreSQL 17**;
- В командной строке соберём PostgreSQL‑аддон **pg\_vector** через x64 Native Tools Command Prompt for VS 2022;
- Протестируем RAG‑систему **20 вопросами к вымышленной документации**, сгенерированной в Gemini 2.5 Pro;
- Узнаем, **во сколько раз медленнее** запускать через CPU, чем через GPU.

---

1\. [LibreChat: как настроить локальный мультичат для сотен нейросетей без подписок (Win11/Linux/Mac)](https://habr.com/ru/companies/bothub/articles/949514/)  
**2\. Как подключить к LibreChat RAG‑систему, чтобы загружать.pdf,.docx,.xlsx и.pptx**

---

Эта статья — [продолжение предыдущей](https://habr.com/ru/companies/bothub/articles/949514/), и если вы уже ставили LibreChat, то сейчас самое время прокачать его.

1. **Прикрепление текстов и документов** (.txt,.csv,.tsv, исходный код,.pdf,.docx,.xlsx,.pptx и др.). Из коробки LibreChat этого не умеет — только изображения.
2. **Поддержка RAG (retrieval‑augmented generation)** — режима, в котором модель способна использовать вашу собственную базу знаний.

> Если хотите меньше танцев с бубном — [попробуйте Open WebUI](https://habr.com/ru/companies/bothub/articles/953986/). Он устанавливается легче и из коробки поддерживает больше форматов файлов.

---

## Что такое RAG, или Как превратить языковую модель в отраслевого эксперта

RAG — это *retrieval‑augmented generation*, или, по‑русски, **«генерация, дополненная извлечённой информацией»**. Представьте, что у модели есть возможность подсмотреть в конспект перед ответом. Именно этим RAG и занимается — добавляет к её внутренним знаниям свежие данные из ваших документов, PDF‑файлов, статей, баз данных, иногда даже из новостей.

### Как же работает эта магия?

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/c58/849/508/c588495086a247dc06fb38a14bb73643.png)

Если отбросить мистику, всё довольно логично. Процесс делится на четыре ключевых этапа:

1\. Документы **индексируются**, примерно как в поисковике. (Да, и это еще один повод сравнить RAG с мини поисковой системой.)

Как часто это происходит? Возможны варианты:

- Если RAG существует в виде «неизменной» базы данных, индексацию делают изредка — при создании базы или обновлении её файлов.
- Если же вы каждый раз прикрепляете к диалогу с чат‑ботом новые файлы в режиме RAG, они будут индексироваться прямо на месте, пока эмбеддинг‑модель работает в фоне. Именно этот способ мы применим в сегодняшней статье.

2\. Когда пользователь задает вопрос, система не сразу отправляет его в языковую модель. Вместо этого она сначала прогоняет запрос через этап **извлечения**. Специальный алгоритм векторного поиска молниеносно ищет в вашей базе знаний фрагменты текста, наиболее релевантные вопросу. Похоже на то, как искать нужную цитату в книге по ключевым словам.

3\. Этап **дополнения**. Найденные фрагменты подшиваются к исходному запросу — и получается расширенный промпт.

Примерно так:

> ВОТ ВОПРОС ПОЛЬЗОВАТЕЛЯ:  
> \[вопрос\]
> 
> ЭТО ВЫДЕРЖКИ ИЗ НАШЕЙ БАЗЫ ЗНАНИЙ:  
> \[контекст\]
> 
> ОТВЕТЬ, ИСПОЛЬЗУЯ ЭТУ ВСПОМОГАТЕЛЬНУЮ ИНФОРМАЦИЮ.

(Чаще используют не капслок, а XML‑теги — модели обрабатывают этот формат стабильнее.)

4\. **Финальный этап — генерация.** Весь собранный промпт отправляется в конечную модель. Её задача — синтезировать связный, аргументированный ответ, как будто это делал эксперт, знающий контекст изнутри. В результате вы получаете аргументированный ответ, основанный на реальных документах.

> Пункты 1–2 (индексация и извлечение) можно выполнять как через CPU, так и через GPU. GPU даёт фору — иначе скорость падает в 5–100 раз (зависит от размера документа, модели поиска и скорости оборудования).
> 
> И ещё приятная деталь: всё это может работать не только локально, но и в облаке.

В итоге RAG — это мощная **надстройка**, благодаря которой можно обходиться без переобучения модели под каждые новые данные. Он превращает универсального собеседника в узкопрофильного специалиста, который в курсе последних событий и ваших внутренних правил.

И теперь попробуем разобраться, как подключить всё это к программе LibreChat.

## Стек

Для этой части вам понадобятся следующие инструменты (пока не устанавливайте — просто ознакомьтесь с набором):

- **Python** версии 3.8+.
- **PostgreSQL**  — скачиваем отсюда: [https://www.postgresql.org/download/](https://www.postgresql.org/download/).
- Расширение для PostgreSQL **pg\_vector**: [https://github.com/pgvector/pgvector.git](https://github.com/pgvector/pgvector.git).
- Проект **rag\_api**: [https://github.com/danny‑avila/rag\_api](https://github.com/danny-avila/rag_api).
- **x64 Native Tools Command Prompt for VS 2022**  — можно установить или обновить по ссылке: [https://visualstudio.microsoft.com/downloads/#build‑tools‑for‑visual‑studio-2022](https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2022).

Локации, в которые будет происходить установка:

- Виртуальное окружение: `C:\rag_env\`;
- RAG‑сервер: `D:\rag_api\`;
- pgvector: `D:\pgvector\`;
- PostgreSQL: `C:\Program Files\PostgreSQL\`;
- Python 3.12: `C:\Python312\`.

## Установка

1\. Устанавливаем **PostgreSQL** отсюда: [https://www.postgresql.org/download/](https://www.postgresql.org/download/).

2\. В процессе установки нужно будет придумать пароль суперпользователя.

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/93e/1ba/15f/93e1ba15fb815d810fd76d2ba77f0f3a.png)

3\. Установите или обновите [**Build Tools for Visual Studio**](https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2022). Эти инструменты нужны, чтобы собрать расширение pg\_vector.

4\. Битва с командной строкой стартует здесь.

Клонируйте проект **pgvector**: `git clone https://github.com/pgvector/pgvector.git D:\pgvector`.

5\. Через «Пуск» запустите **x64 Native Tools Command Prompt for VS 2022** — специальную консольную среду, которая активирует нужные переменные. (Этот ярлык откроет.bat‑файл: `%comspec% /k "C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build\vcvars64.bat"`.)

Кроме того, есть вариант запустить через новую вкладку:

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/687/ee2/71e/687ee271e900923cf4fc5fbe759beb10.png)

6\. В открывшейся среде собираем pg\_vector:

`set "PGROOT=C:\Program Files\PostgreSQL\17" && cd /d D:\pgvector && nmake /F Makefile.win && nmake /F Makefile.win install`

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/fa5/a0d/8e1/fa5a0d8e140959583e14d8ea6f8f7bbd.png)

После успешного запуска появятся свежесобранные файлы расширений в этих папках:

- `C:\Program Files\PostgreSQL\17\share\extension\`
- `C:\Program Files\PostgreSQL\17\include\server\extension\vector\`

7\. Откройте консольный PosgreSQL (**psql**) от имени суперпользователя `postgres`:

`"c:\Program Files\PostgreSQL\17\bin\psql.exe" -U postgres -h localhost -p 5432`

Если запросит, вводим ранее заданный пароль и жмём Enter. Пароль не отображается при вводе, чтобы никто не узнал, сколько звёздочек вбивать.

8\. Находясь в psql, создадим для RAG базу данных и виртуального пользователя. С помощью такой команды:

`CREATE DATABASE rag_db; CREATE USER rag_user WITH PASSWORD 'rag_password_123456' SUPERUSER;`

Да, пароль можно заменить на свой, одновременно нужно будет менять его и в файле `D:\rag_api\.env`.

9\. Клонируйте проект **rag\_api**:

`git clone https://github.com/danny-avila/rag_api D:\`

10\. Теперь займёмся Python. Я устанавливал на **Python 3.12**, взять можно отсюда: [https://www.python.org/downloads/](https://www.python.org/downloads/).

Лучше ставить в папку с коротким путём, например `C:\Python312\`. Включите *Add python.exe to PATH*, иначе команда py не найдёт нужную версию.

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/ed7/414/c3b/ed7414c3b50a2d2ffe6f58bc52fc43fe.png)

11\. Создаём и активируем виртуальное окружение, затем устанавливаем зависимости:

`py -3.12 -m venv C:\rag_env && C:\rag_env\Scripts\activate && cd /d D:\rag_api && pip install -r requirements.txt`

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/ed3/174/264/ed317426418a66e5164be78163cfa279.png)

12\. Создайте файл `D:\rag_api\.env`, следующего содержания:

```
VECTOR_DB_TYPE=pgvector
POSTGRES_DB=rag_db
POSTGRES_USER=rag_user
POSTGRES_PASSWORD=rag_password_123456
DB_HOST=localhost
DB_PORT=5432
DB_NAME=rag_db
COLLECTION_NAME=ragapicollection

# Перед началом индексации данные делятся на фрагменты длиной 1500 символов,
# с перехлестом в 100 символов. Эти значения можно изменить ниже.
CHUNK_SIZE=1500
CHUNK_OVERLAP=100

# Указываем провайдера и модель для эмбеддингов.
# Для intfloat/multilingual-e5-base (≈110M параметров)
# требуется 1,2–1,5 RAM/VRAM.
# Примеры других мультиязычных моделей, которые попробовать:
# - sentence-transformers/all-MiniLM-L6-v2 (≈22M параметров, ~0,4–0,6 ГБ RAM/VRAM)
# - BAAI/bge-large-zh/bge-large-m3 (≈330M параметров, ~3,5–4,5 ГБ RAM/VRAM)
EMBEDDINGS_PROVIDER=huggingface 
EMBEDDINGS_MODEL=intfloat/multilingual-e5-base

# Параметры сервера RAG API:
RAG_HOST=0.0.0.0
RAG_PORT=8000
```

13\. И наконец, подключаем RAG‑сервер к LibreChat’у. Откройте `D:\LibreChat\.env` и добавьте строку:

`RAG_API_URL=http://0.0.0.0:8000`

Всё, подготовка закончена. Дальше начинается запуск и тестирование.

## Запуск

Пора запускать сервер, пробуем сделать это следующей командой:

`cd /d D:\rag_api && C:\rag_env\Scripts\python.exe ‑m uvicorn main:app ‑host 0.0.0.0 ‑port 8000`

(Если хочется удобства — в Windows можно создать ярлык «RAG‑сервер для LibreChat», который делает то же самое: `cmd /k "cd /d D:\rag_api && C:\rag_env\Scripts\python.exe -m uvicorn main:app --host 0.0.0.0 --port 8000"`.)

При первом запуске скачается эмбеддинг‑модель с Hugging Face:

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/b7d/4ba/0d0/b7d4ba0d018f4d3fd717ddbe8d437895.png)

В конце концов окно запущенного сервера выглядит так:

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/5ac/87c/2d6/5ac87c2d62563210849b6e110e460893.png)

---

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/1ba/8a3/b30/1ba8a3b301d1852365d926c5cc9d6678.png)

Пока активен запущенный в командной строке RAG‑сервер, в LibreChat становится возможным прикреплять (вдобавок к изображениям) тексты и документы.

С одним значительным «но»...

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/09e/fde/4b4/09efde4b4f83585af11067a32852760b.png)

Как уже замечено в прошлой статье, RAG — это не просто *один* из способов загрузки офисных файлов в LibreChat, а, по сути, *единственный* способ их загрузить. Соглашусь, что это не очень удобно (и данная проблема обсуждается [в ветвях репозитория](https://github.com/danny-avila/LibreChat/discussions/3367)), поэтому не будет лишним повторить, что [все дороги ведут в Open WebUI](https://habr.com/ru/companies/bothub/articles/953986/).

Наглядный пример XML-разметки, в которой файлы передаются в конечную ИИ-модель

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/118/e4b/25f/118e4b25ff67e256edbd8ddd39107f2a.png)

## Тест RAG-функций

Чтобы проверить систему, я [сгенерировал документацию](https://pastebin.com/hyK11DS3) через Gemini 2.5 Pro — объём примерно ~32 500 символов.

Дальше тест был прост: я прикреплял этот Markdown‑файл к LibreChat и задавал вопросы по его содержимому. Как именно шёл запрос:

- Перетаскивал Markdown‑файл в диалог;
- В промпте задавал конкретный вопрос, относящийся к материалу файла. RAG‑система искала релевантные фрагменты и передавала их вместе с запросом в LLM.
- Дополнительно в промпте просил вывести те самыe фрагменты, которые RAG признала релевантными (то есть, по сути, вернуть невидимую часть запроса обратно), — чтобы прочитать, что именно попало в контекст модели.

Итак — что получилось в тесте. Всего было **20 вопросов**. В качестве конечной модели я использовал **ChatGPT o3-mini** — мини‑модель, быстрая и подходящая для задач с парсингом и быстрыми ответами.

Первая половина вопросов задумывались простыми, с упором на ключевые слова; вторая половина более обобщённые, для углубленного теста RAG‑системы.

Продолжим разбирать результаты и выводы?

### 1.

> **Какова основная цель протокола EtherLink (ELP v4.2) и какие у него ключевые особенности?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/5da/9b5/0be/5da9b50be7186bcc1189a38178feeeb6.png)

RAG уверенно поймал тему, выбрав шесть фрагментов, но слегка расплескал контекст — часть кусков оказалась о портах и JSON‑примерах.

### 2.

> **Что такое когнитивный поток (Cognitive Flow) и какие основные поля содержит его JSON‑структура?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/aaa/f5a/81d/aaaf5a81dd81df2782cd0ffec492020b.png)

Система сработала почти идеально — вытащила фрагменты с определением и JSON‑структурой потока, лишь слегка добавив шум. Ответ получился полный и аккуратный, без выдумок. Типичный случай, когда узкий запрос показывает силу RAG.

### 3.

> **Какие компоненты входят в состав архитектуры эфирного узла AetherCore?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/3a3/913/e2b/3a3913e2bc0673c29af23ff299171c2c.png)

Тут RAG промахнулась — вместо списка компонентов из раздела 3.1 подала общие описания и теорию. LLM, не имея точных данных, ответила частично верно, но поверхностно.

### 4.

> **Как протокол DPoC (Dynamic Proof‑of‑Coherence) обеспечивает консенсус между узлами?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/98c/af1/056/98caf1056e68062a6e33f4f59463e358.png)

Отличный результат — RAG нашел нужный блок с описанием кворума и голосования, а LLM чётко воспроизвела механизм без искажений.

### 5.

> **Что делает модуль безопасности «Эгида» и какие методы шифрования он использует?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/e5b/7f2/9b4/e5b7f29b4ed8656942f2b017deb9d366.png)

RAG попал точно в нужный раздел, между тем добавив пару бесполезных кусков. LLM грамотно структурировала ответ: X.509, AES-256-GCM, ECDSA, всё по тексту. Демонстрация идеального взаимодействия RAG + LLM.

### 6.

> **Какие метрики производительности экспортируются в формате Prometheus?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/657/200/4d8/6572004d868a2c85c8842ae0ef237c1f.png)

RAG‑система вытащила таблицу метрик и контекст, добавив чуть‑чуть мусора из JSON‑разделов. LLM корректно пересказала все показатели и типы. Простая задача — чистый тест на факт‑ретрив, выполнен уверенно.

### 7.

> **Что представляет собой AetherVM и зачем используется язык AetherScript?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/619/9a1/e5e/6199a1e5e69bf70cd6f6feda200e954f.png)

Три из пяти фрагментов в точку, два — шум. Видно, что RAG вытянул глоссарий и примеры, но пропустил детали про байт‑код. Ответ ChatGPT o3-mini тем не менее — краткий, точный, с минимумом воды. Рабочий кейс с лёгким недоборчиком контекста.

### 8.

> **Какие типы топологий поддерживает сетевая модель AetherCore?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/68f/690/f37/68f690f3720839f3cb0c62b49581f3f9.png)

Промах RAG — ключевой раздел с явным списком топологий пропущен. Модель логично додумала на основе общих описаний. Результат выглядит умно, но по факту неточен.

### 9.

> **В чём заключается отличие AetherCore от традиционных HPC и облачных архитектур?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/472/0fe/785/4720fe7855b219081fc96c8a8bd82ed9.png)

Почти эталон: RAG достал таблицу сравнений и нужные куски, LLM выдала структурированный список отличий без ошибок. Небольшие потери деталей, но ответ читается как готовый фрагмент статьи.

### 10.

> **Какие команды доступны в интерфейсе командной строки ACLI?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/1f9/5c3/cb7/1f95c3cb782a9d5e9f9ae6d7698055a8.png)

Один фрагмент попал точно в цель, остальные — шумовые. Несмотря на это, LLM выдала стопроцентно корректный список команд. Хороший пример, как даже при грязном контексте модель может фильтровать мусор.

### 11.

> **Каким образом теория динамических когерентных полей влияет на маршрутизацию когнитивных потоков?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/928/882/968/9288829689aa663833838e51b9af9858.png)

Большинство фрагментов релевантны, ответ математически точен и понятен.

### 12.

> **Почему модель Hybrid Distributed Computing (HDC) считается «гибридной» и какие преимущества это даёт для NP‑трудных задач?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/e3c/39d/e16/e3c39de1682a918ca43431e888c06637.png)

RAG‑система уверенно справилась с вопросом о доступных API‑интерфейсах, вытянув правильные разделы из документации. Ответ получился точным и почти без шума, хотя часть фрагментов касалась лишь косвенно SDK и могла быть отброшена.

### 13.

> **Как система AetherCore реагирует на каскадный отказ узлов и какие механизмы обеспечивают самовосстановление сети?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/b6e/d0e/d3b/b6ed0ed3bb7733dfc091b4337123f346.png)

Тест показал, что RAG может чётко доставать структурированные таблицы и YAML‑примеры. Модель верно воспроизвела поля логов и уровни серьёзности.

### 14.

> **Чем квантовые шарды (QSh) отличаются от реальных квантовых компьютеров и какие задачи на них решаются?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/630/415/2ce/6304152cef881ba1a0e58c47788e87ad.png)

Здесь система немного споткнулась: RAG пропустил ключевой подпункт с пошаговым сценарием, из‑за чего LLM ответила в общих чертах. Тем не менее общая логика и терминология были выдержаны, а конечная ИИ‑модель o3-mini честно призналась, что подробностей в тексте нет. Хороший пример самокоррекции LLM при неполных данных.

### 15.

> **Как параметр** `**coherence_gain**` **в псевдокоде маршрутизатора влияет на выбор узла?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/921/b70/8b2/921b708b2b183550b5522883512fcb1a.png)

Релевантные фрагменты выбраны почти идеально — RAG вытащил куски из раздела о модульной системе и пакетах, с упоминанием версии протоколов и путей импорта.

### 16.

> **Почему архитектура AetherCore не имеет единой точки отказа (SPOF) и как это реализуется на уровне протоколов?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/ef0/39d/a73/ef039da73170327e51361082506abeca.png)

Очень наглядный кейс: RAG подобрал правильные куски, но добавил немного «фонового текста» про мониторинг. LLM аккуратно отфильтровала шум, объяснив механизм приоритизации и SLA‑метрики. Система показала, что даже с частично релевантным контекстом может выдать связный технический ответ.

### 17.

> **Как связаны между собой компоненты NodeManager, FlowDispatcher и Cognitive Router в процессе обработки потока?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/658/f12/b0d/658f12b0d6008c46aa5e2613be296100.png)

Ответ чёткий, но неполный: не хватает упоминания rollback‑процедур. Пример, когда RAG работает «вширь», но не «вглубь».

### 18.

> **Каким образом AetherTrace может быть использована для аудита безопасности или SLA‑анализа?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/3f5/570/15f/3f557015f9aafbb44cb904890ddd1f6d.png)

Система показала стабильность: выбрала нужные фрагменты про REST и gRPC‑шлюзы, а LLM логично структурировала их в список. Мелкий минус — не были подхвачены детали авторизации.

### 19.

> **Если в сети наблюдается рост энтропии** `**E(S)**` **в формуле когерентности, как это повлияет на производительность и поведение системы?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/f05/77c/322/f0577c3229f3c8c7c29c46b4e77a458d.png)

RAG вытащил в целом правильные куски про формулу, но добавил немного шума из соседних разделов. Модель корректно объяснила, как рост энтропии снижает согласованность потоков. Ответ точный, но без деталей про веса α, β, γ — чувствуется лёгкая неполнота.

### 20.

> **Каким образом AetherCore может быть применён в задачах управления городской инфраструктурой, согласно кейсам документа?**

Скриншот

![](https://habrastorage.org/r/w780/getpro/habr/upload_files/df5/7ea/465/df57ea4653b3a4b33489895d4afc6fbe.png)

Система попала почти идеально: подтянула кейсы с интернетом вещей и транспортом, а LLM связала архитектуру с реальными сценариями.

---

### Обобщённый анализ

> Снова подчеркну: всё, что было выше, — про конкретную эмбеддинг‑модель intfloat/multilingual‑e5-base. Именно от **эмбеддингов** на 90% зависит точность RAG‑поиска. Я специально выбрал простую, но быструю модель, чтобы показать, насколько сильно качество эмбеддингов влияет на результат.
> 
> Если бы вместо неё мы взяли, например, BAAI/bge‑large‑zh/bge‑large‑m3 (или что‑то ещё более тяжёлое — вплоть до облачного варианта), картина была бы иной: точность заметно выше, но и потребление памяти выросло бы в разы. А без GPU обработка небольшого документа составила бы несколько минут.

В общем и целом ответы LLM **устойчиво качественные:** при точных фрагментах, предоставленных RAG, — исчерпывающие, при шумных — аккуратные обобщения без галлюцинаций. Видно, что модель неплохо фильтрует шум и даже при неидеальных данных ChatGPT o3-mini сохраняет структуру и корректность.

Повторяется закономерность — узкие вопросы обрабатываются лучше широких. RAG честно вытаскивает релевантные чанки, но иногда перестраховывается и добавляет соседние абзацы. Где вопрос узкий и формулировка конкретная, точность на уровне 85–95%.

**И наконец,** тесты показали, что в текущем виде это уже рабочий инструмент для поиска по техдокам, помощник для инженеров и исследователей, а также отличный полигон, чтобы протестировать параметры RAG.

---

Таким образом, надстройка RAG — это не просто технический трюк, а стратегическое улучшение, которое кардинально расширяет сферу применения чат‑бота. Он перестает быть просто генератором текста и становится интеллектуальным интерфейсом к вашей персональной или корпоративной базе знаний.

Проведенное тестирование на 20 вопросах наглядно продемонстрировало сильные и слабые стороны системы. Это доказывает, что RAG — настраиваемый инструмент, у которого есть несколько переменных (включая используемые модели, размеры фрагментов и выбор между локальным или облачным запуском).

---

В статье я применил модель ChatGPT o3-mini. Вы тоже можете протестировать её и многие другие [в сервисе BotHub](https://bothub.chat/?invitedBy=m_aGCkuyTgqllHCK0dUc7), эта ссылка даёт бонусные 100 000 капсов.

Только зарегистрированные пользователи могут участвовать в опросе. [Войдите](https://habr.com/kek/v1/auth/habrahabr/?back=/ru/companies/bothub/articles/956892/&hl=ru), пожалуйста.

А как вы предпочитаете прокачивать своего ИИ-ассистента?

75% Только RAG, дообучение — это сложно 3

0% Только дообучение (файнтюнинг) 0

50% Комбинирую оба подхода в зависимости от задачи 2

25% Мне хватает стандартных возможностей без всякой прокачки 1

0% Я использую другие методы (напишу в комментариях) 0